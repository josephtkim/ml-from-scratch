# ğŸ“˜ PCA: Overview

---

### ğŸ¯ Goal

Reduce high-dimensional data to a lower-dimensional subspace while preserving the most **important structure** â€” specifically, the directions of **maximum variance**.

Given:

- **Input data matrix**:  
  `X âˆˆ â„â¿Ë£áµˆ`  
  where `n` is the number of samples and `d` is the number of features

We aim to find a set of **orthonormal vectors** (principal axes):

- `W âˆˆ â„áµˆË£áµ`, with `k < d`

so that the data projected onto this new subspace retains the most variance.

---

### ğŸ“‰ Loss / Objective

PCA solves the following optimization problem:

> **Minimize reconstruction error**:  
>  
>â€ƒâ€ƒ`L = ||X - X_proj||Â²_F = ||X - X W Wáµ€||Â²_F`

Or equivalently:

> **Maximize variance along projections**:  
>  
>â€ƒâ€ƒmaximize `Tr(Wáµ€ Î£ W)`  
>â€ƒâ€ƒsubject to `Wáµ€ W = I`

Where:

- `Î£ = (1/n) Xáµ€ X` is the empirical covariance matrix (after centering `X`)
- `W` consists of the top-`k` eigenvectors of `Î£`

Thus, PCA is both a **compression** and a **variance-preserving projection** method.

---

### ğŸ§  What's Optimized

PCA optimizes for a **projection matrix `W`** such that:

- The columns of `W` (principal components) are **orthonormal**  
  `Wáµ€ W = I`
- The directions in `W` capture the **largest variance** in the dataset

This is solved by computing the **eigen-decomposition** of the covariance matrix.

---

### ğŸ§® Steps to Compute PCA

1. **Center the data**:  
   Subtract the mean of each feature from the data.  
   `X_centered = X - Î¼`

2. **Compute the covariance matrix**:  
   `Î£ = (1/n) X_centeredáµ€ X_centered`

3. **Eigen-decompose** the covariance matrix:  
   `Î£ = V Î› Váµ€`

4. **Sort eigenvalues** `Î»áµ¢` in descending order

5. **Select top-`k` eigenvectors**:  
   `W âˆˆ â„áµˆË£áµ`

6. **Project the data**:  
   `X_reduced = X_centered â‹… W`

---
