# ğŸ“˜ Multilayer Perceptron â€” Background

---

### ğŸ¯ Goal

To approximate complex **non-linear functions** by stacking fully connected layers with learnable weights and non-linear activation functions.

Given:
- Input `x âˆˆ â„áµˆ`
- Layer sizes: `[nâ‚€, nâ‚, ..., n_L]` where `nâ‚€ = d` (input dim), `n_L` = output dim

For each layer `â„“ = 1, ..., L`:
1. **Linear transformation**:

   ```
   z[â„“] = W[â„“] Â· a[â„“âˆ’1] + b[â„“]
   ```

2. **Apply non-linearity**:

   ```
   a[â„“] = Ïƒ(z[â„“])
   ```

Where:
- `a[0] = x` (the input)
- `W[â„“] âˆˆ â„â¿Ë¡ Ã— â¿Ë¡â»Â¹` (weights)
- `b[â„“] âˆˆ â„â¿Ë¡` (biases)
- `Ïƒ`: activation function (e.g. ReLU or softmax)

The final output `Å· = a[L]` is the prediction.

---

### ğŸ“‰ Loss / Objective

For **classification** (e.g. softmax output), use **cross-entropy loss**:

```
L = âˆ’âˆ‘ yáµ¢ log(Å·áµ¢)
```

For **regression**, use **mean squared error (MSE)**:

```
L = (1/N) âˆ‘ (yáµ¢ âˆ’ Å·áµ¢)Â²
```

---

### ğŸ§  What's Optimized

The model learns:
- Weights `W[â„“]` and biases `b[â„“]` for all layers
- Optimization is done by minimizing the loss `L` using **gradient descent**

Using **backpropagation**, gradients are computed recursively:

- For the **output layer**:

  ```
  Î´[L] = âˆ‡â‚ L âŠ™ Ïƒâ€²(z[L])
  ```

- For **hidden layers**:

  ```
  Î´[â„“] = (W[â„“+1]áµ€ Â· Î´[â„“+1]) âŠ™ Ïƒâ€²(z[â„“])
  ```

Parameter updates (for all layers `â„“`):

```
W[â„“] â† W[â„“] âˆ’ Î· Â· âˆ‚L/âˆ‚W[â„“]
b[â„“] â† b[â„“] âˆ’ Î· Â· âˆ‚L/âˆ‚b[â„“]
```

Where:
- `Î·` is the learning rate
- `Î´[â„“]` is the error signal at layer `â„“`
- `âŠ™` is element-wise (Hadamard) product

---
