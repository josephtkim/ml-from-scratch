# Machine Learning From Scratch

## ✅ Learning Plan & Progress

### **Part 1 – Supervised Learning & Foundations**
- [ ] Linear Regression (closed-form + gradient descent)
- [ ] Logistic Regression (binary + multinomial)
- [ ] k-Nearest Neighbors
- [ ] Naive Bayes (text classification)
- [ ] Decision Trees
- [ ] Random Forests
- [ ] Support Vector Machines
- [ ] Gradient Boosted Trees
- [ ] Perceptron
- [ ] Multilayer Perceptron (backprop in NumPy)

---

### **Part 2 – Deep Learning & Unsupervised**
- [ ] CNN (LeNet-style)
- [ ] Activation function experiments
- [ ] RNN (char-level)
- [ ] LSTM / GRU
- [ ] k-Means
- [ ] PCA
- [ ] Gaussian Mixture Models (EM)
- [ ] Autoencoder
- [ ] Variational Autoencoder
- [ ] t-SNE visualization

---

### **Part 3 – Advanced Models & RL**
- [ ] GAN (MNIST)
- [ ] DCGAN
- [ ] Attention (scaled dot-product)
- [ ] Transformer Encoder (classification)
- [ ] Tiny GPT (char-level LM)
- [ ] Multi-Armed Bandits (ε-greedy, UCB, Thompson)
- [ ] Value Iteration & Policy Iteration
- [ ] Q-Learning
- [ ] SARSA
- [ ] DQN (CartPole)
- [ ] Policy Gradient (REINFORCE)
- [ ] Actor-Critic (lite A2C)

---

### Stretch Goals
- [ ] Graph Neural Networks (GCN)
- [ ] Normalizing Flows
- [ ] Diffusion Models (toy denoising model)
- [ ] Meta-Learning (MAML)

---

## Author
**Joseph Taewoo Kim**  
- B.S. Mathematics, Minor in Physics  
- SWE @ Meta (ML Infrastructure)  
- Aiming to bridge **mathematics, ML, and research**
